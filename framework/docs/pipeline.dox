/*! \page pipeline_config_page Pipeline and Module Basics

\section pipe_overview Overview
A pipeline in the TSK framework is simply a set of modules that are run in a specific order.  Modules will be covered in more detail in the next major section, but for now all we need to know is that a module does a specific type of analysis.  An example module could calculate the MD5 hash of a file and another module could lookup the hash value in a database to see if it is known.  Modules can communicate between each other and the MD5 hash would be passed from the first to the second. 

A pipeline is configured in an XML file, which is described later.

\section pipe_types File Analysis versus Reporting Pipelines

The framework currently supports two types of pipelines.  Each is used in a different context and this section will outline the intended uses of each.  

The first type of pipeline is a file analysis pipeline and it is intended to be run on every file that is found in a disk image.  The second type of pipeline is called a reporting pipeline (or post-processing pipeline) and it is intended to be run after all of the individual files are analyzed. 

A file analysis pipeline allows you to perform a task on every file in the system.  Each module  in the pipeline is passed a reference to a file object through which it can access both the metadata and content of the file.  The module can also access the analysis results of previously run modules using the \ref bb_overview.  Examples of file analysis modules include hash calculation, hash lookup, archive file extraction, and text extraction.

A reporting pipeline allows you to perform a task after all of the file analysis modules have been run and all files have been analyzed.  There are two main uses for these modules.  One is that they can compile the results from the individual file analysis modules into a single report (hence the name).  The other use, is that it is a more efficient method for analyzing a small subset of files in the system.  For example, if you have a Windows registry analysis module, it would be more efficient to run that as a reporting module and it will simply locate the few hive files and analyze them.  If the registry analysis module were a file analysis module, it would be run on every file in the system and 99% of the time it would decide to ignore the file because it wasn't a registry hive. 

\section pipe_modtypes Dynamic Library versus Executable Modules
There are two major types of modules that can exist in either type of pipeline. One is a dynamic library module and the other is an executable module.  

Dynamic library modules are programmed specifically for inclusion into the framework.  These modules can access all of the framework resources.  Creating one of these modules is described in \ref mod_devpage.

Executable modules are simply command line tools that the pipeline runs.  If the tool can be run from the command line, then it can be run from within the pipeline.  The configuration file will allow you to pass in various arguments.  Executable modules do not have access to the database, blackboard, and other services that the framework provides.  That means that if you want the results from an executable module to be included back into the framework for later modules to see, you'll still need to make a dynamic link module to parse the results and add them to the blackboard.

\section pipe_config Pipeline Configuration

\subsection pipe_config_file Configuration File
Both file analysis and reporting pipelines are configured through an XML file.  A single XML file can store both a file analysis and a reporting pipeline.  The following is an example of a pipeline configuration file:

<pre>&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;PIPELINE_CONFIG&gt;    &lt;PIPELINE type="FileAnalysis"&gt;        &lt;MODULE order="1" type="plugin" location="CalcFileSizeModule.dll"/&gt;    &lt;/PIPELINE&gt;
    &lt;PIPELINE type="Report"&gt;        &lt;MODULE order="1" type="plugin" location="SummaryReport.dll" arguments="@OUT\Summary.htm"/&gt;    &lt;/PIPELINE&gt;&lt;/PIPELINE_CONFIG&gt;
</pre>

Each module is represented by a MODULE entry in the configuration file consisting of the following attributes:

<table>
<tr><th>Attribute</th><th>Description</th></tr>
<tr><td>order</td><td>The position of the module within the pipeline.</td></tr><tr><td>type</td><td>Either "executable" or "plugin".</td></tr><tr><td>location</td><td>The path to either the executable or dynamic library file to be run. This can either be a fully qualified or relative path. If relative, the framework will look for the file in the system path or the located specified in TskSystemPropertiesImpl::MODULE_DIR in the TskSystemProperties service.</td></tr><tr><td>arguments</td><td>The arguments to pass to the module.  These will be parsed by the module.  See the section below on using macro substation to pass in file-specific details.</td></tr> <tr><td>output</td><td>The path to a file where output from the module should be written. This attribute applies only to executable modules.   See the section below on using macro substation to pass in file-specific details. </td></tr>
</table>

When configuring a pipeline module pay particular attention to the following details: - Module ordering must start at 1 and must be sequential (i.e. you must not have any gaps in the list of modules). - Redirected output will be appended to the specified output file.  - Attempting to write output to a shared file may result in file access errors when multiple framework sessions (in a distributed environment) attempt to write data to the same file. You can avoid this by using the \@UNIQUE_ID macro. - You must escape the following characters if you wish to include them in the command line:

<table><tr><th>Character</th><th>Escaped Character</th></tr><tr><td>&amp;</td><td>&amp;amp;</td></tr><tr><td>&quot;</td><td>&amp;quot;</td></tr><tr><td>&gt;</td><td>&amp;gt;</td></tr><tr><td>&lt;</td><td>&amp;lt;</td></tr><tr><td>&apos;</td><td>&amp;apos;</td></tr>
</table>
The Validate_Pipeline.exe tool, which comes with the framework, can be used to verify that the pipeline configuration file is correct and that all modules can be found.


\subsection pipe_config_macro Macro Substitution

The "arguments" and "output" attributes in the configuration file allow for the substitution of run-time values.  Refer to the TskModule documentation for the full list (such as TskModule::FILE_MACRO), but some examples are given here. 

Placing &quot;\@FILE&quot; in the attribute will cause the pipeline to replace it with the unique ID that the file has been assigned.  Or, &quot;\@OUT&quot; will be replaced by the preferred output folder (as defined by the program using the pipeline).  

*/
