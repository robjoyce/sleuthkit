/*! \page pipeline_config_page Pipeline and Module Basics

\section pipe_overview Overview
A pipeline in the TSK framework is simply a set of modules that are run in a specific order.  
Modules are covered in more detail in \ref mod_devpage, but for now all we need to know is that a module does a specific type of analysis.  
For example, one module could calculate the MD5 hash of a file and another module could look up the hash value in a database to see if it the file is a known file.  
Modules can communicate with each other, so the MD5 hash could be passed from the first module to the second module. 

Pipelines are configured using an XML file, which is described later.

\section pipe_types File Analysis vs. Post-Processing Pipelines

The framework currently supports two types of pipelines: file analysis pipelines and post-processing pipelines.  
Each type of pipeline is used in a different context.  

A file analysis pipeline allows you to perform tasks on every file in an image.  
Each module in this type of pipeline is passed a reference to a file object through which it can access both the metadata and content of the file.  
The module can also access the analysis results of previously run modules using the blackboard.  
Examples of file analysis modules include modules to do hash calculation, hash lookup, archive file extraction, and text extraction.

A post-processing pipeline allows you to perform tasks after all of the file analysis modules have been run and all files have been analyzed.  
There are two main uses for this type of pipeline.  
First, post-processing modules can compile the results from the individual file analysis modules into a single analysis, perhaps writing a report.  
Second, a post-processing module is a more efficient mechanism for analyzing a small subset of the files in an image.  
For example, if you need a Windows registry analysis module, it would be better to develop it as a post-processing module that simply locates the handful of registry hive files in an image and analyzes them.  
If the registry analysis module was instead developed as a file analysis module, it would be run for every file in the image and most of the time it would decide to ignore the file because it wasn't a registry hive. 

\section pipe_modtypes Plug-In vs. Executable Modules
There are two major types of modules that can exist in either type of pipeline. One is a dynamic linked library (DLL) or plug-in module and the other is an executable (EXE) module.  

Plug-in modules are programmed specifically for inclusion into the framework.  These modules can access all of the framework resources.  What's required to create one of these modules is described in \ref mod_devpage.

Executable modules are simply command line tools that a pipeline runs.  
If the tool can be run from the command line, then it can be run from within the pipeline.  
The configuration file will allow you to pass in a string of arguments to pass through to the executable.  
However, executable modules do not have access to the database, blackboard, and other services that the framework provides.  
This means that if you want the results from an executable module to be available to other modules, you'll still need to make a companion plug-in module to parse the results and add them to the blackboard.

\section pipe_config Pipeline Configuration

\subsection pipe_config_file Pipeline Configuration Files
Both file analysis and post-processing pipelines are configured using an XML file.  
A single XML file can store the configuration of both a file analysis and a post-processing pipeline.  
Take a look at \ref sample_pipeline_config_file_page to see an example of a pipeline configuration file.

Note that each module entry in the pipeline is represented by a MODULE element in the configuration file.  Each MODULE element can have the following XML attributes:

<table>
<tr><th>Attribute</th><th>Description</th></tr>

<tr><td>order</td><td>The position of the module within the pipeline. </td></tr>
<tr><td>type</td><td>Either "executable" or "plugin".</td></tr>
<tr><td>location</td><td>The path of the executable to be run by an executable module or the DLL to be loaded for a plug-in module. This can either be a fully qualified or relative path. If the path is relative, the framework will look for the file in the system path or the location specified as TskSystemProperties::MODULE_DIR in the TskSystemProperties service.</td></tr>
<tr><td>arguments</td><td>The arguments to pass to the module.  See \ref pipe_config_macro.</td></tr> 
<tr><td>output</td><td>The path to a file where output from the module should be written. This attribute applies only to executable modules.  See \ref pipe_config_macro.</td></tr>

</table>


When configuring a pipeline module pay particular attention to the following details: 
- Module ordering does not need to be sequential (i.e., there can be gaps), but you cannot have two modules with the same order value.

- Redirected output on executable modules will be appended to the specified output file. 

- Attempting to write output to a shared file may result in file access errors when multiple framework sessions (in a distributed environment) attempt to write data to the same file. 
You can avoid this by using the TskSystemProperties::UNIQUE_ID macro to construct the file name (see \ref pipe_config_macro for more on config file macros).

- You must escape the following characters if you wish to include them in the command line:


<table><tr><th>Character</th><th>Escaped Character</th></tr><tr><td>&amp;</td><td>&amp;amp;</td></tr><tr><td>&quot;</td><td>&amp;quot;</td></tr><tr><td>&gt;</td><td>&amp;gt;</td></tr><tr><td>&lt;</td><td>&amp;lt;</td></tr><tr><td>&apos;</td><td>&amp;apos;</td></tr>
</table>


\subsection validate_pipe_config_file Validating Pipeline Configuration Files
The tsk_validatepipeline.exe tool, which comes with the framework, can be used to verify that the pipeline configuration file is correct and that all modules can be found.

\subsection pipe_config_macro Configuration File Macros

The "arguments" and "output" attributes of a MODULE element in a pipeline configuration file allow for the substitution of run-time values into the associated strings. 
This is possible because there is a set of config file macros that the framework expands when it reads in a pipeline configuration file.   

There are a number of system property macros. 
To substitute a system property in a string, surround the system property name with '#' marks.  
Refer to the TskSystemProperties::PredefinedProperty list for the set of official system properties.  
For example, use "#SYSTEM_OUT_DIR#/file1.txt" to refer to a file named "file1.txt" in the system output directory. 

File analysis modules can also employ a macro that expands to the path of the file currently being analyzed. 
Because many files can be analyzed at the same time in a distributed disk image analysis system, the current file is not stored in SystemProperties, but the framework knows how to expand TskModule::CURRENT_FILE_MACRO appropriately.
Note that although this macro is not strictly necessary for plug-in modules because they have access to file metadata through the TskFile objects passed to them by the pipeline, executable modules can only obtain the path to the current file using this macro. 

*/